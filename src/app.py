import streamlit as st
import requests
import json
from typing import List, Dict, Generator
import time
import tiktoken

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Ollama Chatbot",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t√πy ch·ªânh
st.markdown("""
<style>
    .stChatMessage {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    .assistant-message {
        background-color: #f1f8e9;
        border-left: 4px solid #4caf50;
    }
    .sidebar .sidebar-content {
        background-color: #f5f5f5;
    }
</style>
""", unsafe_allow_html=True)

class OllamaClient:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.headers = {"Content-Type": "application/json"}
        # Kh·ªüi t·∫°o tokenizer ƒë·ªÉ ƒë·∫øm token
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except:
            self.tokenizer = None
    
    def count_tokens(self, text: str) -> int:
        """ƒê·∫øm s·ªë token trong text"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        else:
            # Fallback: ∆∞·ªõc t√≠nh 1 token ‚âà 4 k√Ω t·ª±
            return len(text) // 4
    
    def get_models(self) -> List[str]:
        """L·∫•y danh s√°ch models c√≥ s·∫µn t·ª´ Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", headers=self.headers)
            if response.status_code == 200:
                models = response.json()
                return [model["name"] for model in models.get("models", [])]
            return []
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Ollama server: {e}")
            return []
    
    def chat_completion(self, messages: List[Dict], model: str, **kwargs) -> tuple:
        """G·ª≠i y√™u c·∫ßu chat completion t·ªõi Ollama v√† tr·∫£ v·ªÅ (response, token_count)"""
        
        # Chuy·ªÉn ƒë·ªïi messages th√†nh format c·ªßa Ollama
        ollama_messages = []
        for msg in messages:
            ollama_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        payload = {
            "model": model,
            "messages": ollama_messages,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.7),
                "num_predict": kwargs.get("max_tokens", 1000),
                "top_p": kwargs.get("top_p", 0.9),
                "frequency_penalty": kwargs.get("frequency_penalty", 0.0),
                "presence_penalty": kwargs.get("presence_penalty", 0.0),
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                headers=self.headers,
                json=payload,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["message"]["content"]
                token_count = self.count_tokens(content)
                return content, token_count
            else:
                error_msg = f"L·ªói API: {response.status_code} - {response.text}"
                return error_msg, 0
                
        except Exception as e:
            error_msg = f"L·ªói k·∫øt n·ªëi: {str(e)}"
            return error_msg, 0
    
    def stream_chat_completion(self, messages: List[Dict], model: str, **kwargs) -> Generator[str, None, None]:
        """Streaming chat completion v·ªõi Ollama"""
        
        # Chuy·ªÉn ƒë·ªïi messages th√†nh format c·ªßa Ollama
        ollama_messages = []
        for msg in messages:
            ollama_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        payload = {
            "model": model,
            "messages": ollama_messages,
            "stream": True,
            "options": {
                "temperature": kwargs.get("temperature", 0.7),
                "num_predict": kwargs.get("max_tokens", 1000),
                "top_p": kwargs.get("top_p", 0.9),
                "frequency_penalty": kwargs.get("frequency_penalty", 0.0),
                "presence_penalty": kwargs.get("presence_penalty", 0.0),
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                headers=self.headers,
                json=payload,
                stream=True,
                timeout=120
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            json_data = json.loads(line.decode('utf-8'))
                            if not json_data.get("done", False):
                                content = json_data.get("message", {}).get("content", "")
                                if content:
                                    yield content
                            else:
                                break
                        except json.JSONDecodeError:
                            continue
            else:
                yield f"L·ªói API: {response.status_code}"
                
        except Exception as e:
            yield f"L·ªói k·∫øt n·ªëi: {str(e)}"

    def pull_model(self, model_name: str) -> Generator[str, None, None]:
        """Pull model t·ª´ Ollama registry"""
        payload = {
            "name": model_name,
            "stream": True
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/pull",
                headers=self.headers,
                json=payload,
                stream=True,
                timeout=300
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            json_data = json.loads(line.decode('utf-8'))
                            status = json_data.get("status", "")
                            if status:
                                yield status
                        except json.JSONDecodeError:
                            continue
            else:
                yield f"L·ªói khi pull model: {response.status_code}"
                
        except Exception as e:
            yield f"L·ªói k·∫øt n·ªëi: {str(e)}"

def init_session_state():
    """Kh·ªüi t·∫°o session state"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "ollama_client" not in st.session_state:
        st.session_state.ollama_client = None
    if "models" not in st.session_state:
        st.session_state.models = []
    if "current_model" not in st.session_state:
        st.session_state.current_model = None
    if "total_tokens_generated" not in st.session_state:
        st.session_state.total_tokens_generated = 0
    if "connection_status" not in st.session_state:
        st.session_state.connection_status = False

def auto_connect():
    """T·ª± ƒë·ªông k·∫øt n·ªëi t·ªõi Ollama t·∫°i port 11434"""
    if not st.session_state.connection_status:
        with st.spinner("ƒêang k·∫øt n·ªëi t·ªõi Ollama localhost:11434..."):
            st.session_state.ollama_client = OllamaClient("http://localhost:11434")
            st.session_state.models = st.session_state.ollama_client.get_models()
            if st.session_state.models:
                st.session_state.current_model = st.session_state.models[0]  # Ch·ªçn model ƒë·∫ßu ti√™n
                st.session_state.connection_status = True
                st.success(f"K·∫øt n·ªëi th√†nh c√¥ng! Model hi·ªán t·∫°i: {st.session_state.current_model}")
            else:
                st.warning("K·∫øt n·ªëi th√†nh c√¥ng nh∆∞ng kh√¥ng t√¨m th·∫•y model n√†o. Vui l√≤ng pull model tr∆∞·ªõc!")

def main():
    init_session_state()
    
    st.title("ü§ñ Ollama Chatbot")
    st.markdown("---")
    
    # T·ª± ƒë·ªông k·∫øt n·ªëi khi kh·ªüi ƒë·ªông
    auto_connect()
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        # Th·ªëng k√™
        st.subheader("üìä Th·ªëng k√™")
        
        # ƒê·∫øm s·ªë turn (m·ªói c·∫∑p user-assistant = 1 turn)
        user_messages = [msg for msg in st.session_state.messages if msg["role"] == "user"]
        assistant_messages = [msg for msg in st.session_state.messages if msg["role"] == "assistant"]
        turns = min(len(user_messages), len(assistant_messages))
        
        st.metric("S·ªë l∆∞·ª£t h·ªôi tho·∫°i", turns)
        st.metric("T·ªïng tin nh·∫Øn", len(st.session_state.messages))
        st.metric("Token ƒë√£ t·∫°o", st.session_state.total_tokens_generated)
        
        if st.session_state.current_model:
            st.metric("Model hi·ªán t·∫°i", st.session_state.current_model)
        
        st.markdown("---")
        
        # C·∫•u h√¨nh server
        st.subheader("üîß C·∫•u h√¨nh Server")
        server_url = st.text_input(
            "Ollama Server URL",
            value="http://localhost:11434",
            help="URL c·ªßa Ollama server"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ K·∫øt n·ªëi l·∫°i"):
                with st.spinner("ƒêang k·∫øt n·ªëi..."):
                    st.session_state.ollama_client = OllamaClient(server_url)
                    st.session_state.models = st.session_state.ollama_client.get_models()
                    if st.session_state.models:
                        st.session_state.current_model = st.session_state.models[0]
                        st.session_state.connection_status = True
                        st.success(f"K·∫øt n·ªëi th√†nh c√¥ng! Model: {st.session_state.current_model}")
                    else:
                        st.session_state.connection_status = False
                        st.error("K·∫øt n·ªëi th·∫•t b·∫°i ho·∫∑c kh√¥ng c√≥ model")
        
        with col2:
            if st.button("üîÑ Refresh Models"):
                if st.session_state.ollama_client:
                    st.session_state.models = st.session_state.ollama_client.get_models()
                    if st.session_state.models:
                        st.success(f"ƒê√£ t·∫£i {len(st.session_state.models)} models")
                    else:
                        st.warning("Kh√¥ng t√¨m th·∫•y model n√†o")
        
        # Pull model m·ªõi
        st.subheader("üì• Pull Model")
        model_to_pull = st.text_input("T√™n model c·∫ßn pull", placeholder="v√≠ d·ª•: llama3.2:latest")
        if st.button("Pull Model") and model_to_pull:
            with st.spinner("ƒêang pull model..."):
                status_placeholder = st.empty()
                for status in st.session_state.ollama_client.pull_model(model_to_pull):
                    status_placeholder.text(f"Status: {status}")
                st.success("Pull model ho√†n t·∫•t!")
                # Refresh models list
                st.session_state.models = st.session_state.ollama_client.get_models()
        
        # Ch·ªçn model
        if st.session_state.models:
            st.session_state.current_model = st.selectbox(
                "Ch·ªçn Model",
                st.session_state.models,
                index=st.session_state.models.index(st.session_state.current_model) if st.session_state.current_model in st.session_state.models else 0
            )
        
        st.markdown("---")
        
        # C·∫•u h√¨nh generation
        st.subheader("üéõÔ∏è C√†i ƒë·∫∑t Generation")
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.slider("Max Tokens", 100, 4000, 1000, 100)
        top_p = st.slider("Top P", 0.0, 1.0, 0.9, 0.1)
        frequency_penalty = st.slider("Frequency Penalty", -2.0, 2.0, 0.0, 0.1)
        presence_penalty = st.slider("Presence Penalty", -2.0, 2.0, 0.0, 0.1)
        
        use_streaming = st.checkbox("Streaming Response", value=True)
        
        st.markdown("---")
        
        # System prompt
        st.subheader("üí¨ System Prompt")
        system_prompt = st.text_area(
            "System Prompt",
            value="B·∫°n l√† m·ªôt AI assistant th√¥ng minh, h·ªØu √≠ch v√† th√¢n thi·ªán. H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c.",
            height=100
        )
        
        # Clear chat
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat"):
            st.session_state.messages = []
            st.session_state.total_tokens_generated = 0
            st.rerun()
        
        # Model info
        if st.session_state.current_model:
            st.markdown("---")
            st.subheader("‚ÑπÔ∏è Model Info")
            st.text(f"Model: {st.session_state.current_model}")
            st.text(f"Server: {server_url}")
    
    # Main chat interface
    # Hi·ªÉn th·ªã messages
    for message in st.session_state.messages:
        if message["role"] == "user":
            with st.chat_message("user"):
                st.write(message["content"])
        elif message["role"] == "assistant":
            with st.chat_message("assistant"):
                st.write(message["content"])
    
    # Input cho tin nh·∫Øn m·ªõi
    user_input = st.chat_input("Nh·∫≠p tin nh·∫Øn...")
    
    if user_input and st.session_state.connection_status and st.session_state.current_model:
        # Th√™m tin nh·∫Øn user v√†o history
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        # Hi·ªÉn th·ªã tin nh·∫Øn user
        with st.chat_message("user"):
            st.write(user_input)
        
        # Chu·∫©n b·ªã messages cho API
        api_messages = []
        if system_prompt.strip():
            api_messages.append({"role": "system", "content": system_prompt})
        
        api_messages.extend(st.session_state.messages)
        
        # Generate response
        with st.chat_message("assistant"):
            if use_streaming:
                # Streaming response
                response_placeholder = st.empty()
                full_response = ""
                
                for chunk in st.session_state.ollama_client.stream_chat_completion(
                    api_messages,
                    st.session_state.current_model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=top_p,
                    frequency_penalty=frequency_penalty,
                    presence_penalty=presence_penalty
                ):
                    full_response += chunk
                    response_placeholder.write(full_response + "‚ñå")
                
                response_placeholder.write(full_response)
                response = full_response
                # ƒê·∫øm token cho streaming response
                token_count = st.session_state.ollama_client.count_tokens(full_response)
            else:
                # Non-streaming response
                with st.spinner("ƒêang ph·∫£n h·ªìi..."):
                    response, token_count = st.session_state.ollama_client.chat_completion(
                        api_messages,
                        st.session_state.current_model,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        top_p=top_p,
                        frequency_penalty=frequency_penalty,
                        presence_penalty=presence_penalty
                    )
                st.write(response)
        
        # Th√™m response v√†o history v√† c·∫≠p nh·∫≠t token count
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.session_state.total_tokens_generated += token_count
        st.rerun()
    
    elif user_input and not st.session_state.connection_status:
        st.error("Ch∆∞a k·∫øt n·ªëi t·ªõi Ollama Server! Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi.")
    
    elif user_input and not st.session_state.current_model:
        st.error("Ch∆∞a ch·ªçn model! Vui l√≤ng pull model ho·∫∑c ch·ªçn model c√≥ s·∫µn.")

if __name__ == "__main__":
    main()